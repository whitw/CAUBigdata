{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSTRACT\n",
    "This project analyzes Airbnb listings in the city of  New York to better understand how different\n",
    "attributes such as bedrooms, location, house type amongst others can be used to accurately predict\n",
    "the price of listing that is optimal in terms of the host’s profitability yet affordable to their guests. \n",
    "\n",
    "*This model is intended to be helpful to the internal pricing tools that Airbnb provides to its hosts.*\n",
    "\n",
    "### Objective of the PROJECT is to find:\n",
    "- Estimate listing price based on provided amenities\n",
    "- How review scores effect price of listing\n",
    "- how cancellation policy effects price of listing\n",
    "\n",
    "\n",
    "### dataset\n",
    "collected from InsideAirBnB website for NewYork City(NYC) from jan-mar 2020\n",
    "http://insideairbnb.com/get-the-data.html\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "- id - listing identifier that can be used to create a join with other files\n",
    "- last_scraped - data scrapped date\n",
    "- name -name of the listing\n",
    "- host_id -unique id given to host\n",
    "- host_since - joining date of host can be used to calculate host experience based on duration since the first listing\n",
    "- host_is_superhost - categorical t or f - describing highly rated and relaible hosts (https://www.airbnb.co.uk/superhost)\n",
    "- host_identity_verified - categorical t or f - another credibility metric\n",
    "- host_response_rate -  response rate is the percentage of new enquiries and reservation requests you responded to (by either     accepting/pre-approving or declining) within 24 hours in the past 30 days\n",
    "- host_listings_count - total listing host have\n",
    "- neighbourhood_group- Burough of NYC\n",
    "- neighbourhood_cleansed -neighbourhoods in a burough zipcode\n",
    "- latitude - we will use it later to visualise the data on the map\n",
    "- longitude - we will use it later to visualise the data on the map\n",
    "- property_type -description of property ex:appartment,privatehome\n",
    "- room_type - type of room ex:shared room\n",
    "- accommodates - discrete value describing property number people can accomodate\n",
    "- bathrooms - another discrete value describing property\n",
    "- bedrooms - another discrete value describing property\n",
    "- beds - another discrete value describing property\n",
    "- bed_type - categorical value describing property type of bed ex: realbed or couch\n",
    "- amenities - wifi tv dryer so on...\n",
    "- price - price per night for number of included guests\n",
    "- security_deposit - another continous value assiociated with the cost\n",
    "- cleaning_fee - additional cost at the top of rent\n",
    "- guests_included - number of guest that can be allowed on the price\n",
    "- extra_people - cost of additional person per night\n",
    "- minimum_nights - another discrete value that is cost related.Listing with high value of minimum nights are likely sublettings\n",
    "- maximun nights -property availability\n",
    "- availablitiy 365-availability of the listing from scrapped date to next 365 days\n",
    "- first_review - first review date\n",
    "- last_review - last review date\n",
    "- number_of_reviews - total number of reviews in entire listing history\n",
    "- review_scores_accuracy - discrete value - numbers between 2 and 10\n",
    "- review_scores_value - discrete value - numbers between 2 and 10\n",
    "- review_scores_rating - this value is calculated as weighted sum of other scores\n",
    "- reviews_per_month - given reviews in a month \n",
    "- instant_bookable - categorical value - t or false\n",
    "- cancellation_policy - ordinal value with 5 categories that can be ordered from lowest to highest level of flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:12:08.692547Z",
     "iopub.status.busy": "2022-11-25T10:12:08.692121Z",
     "iopub.status.idle": "2022-11-25T10:12:08.698376Z",
     "shell.execute_reply": "2022-11-25T10:12:08.696955Z",
     "shell.execute_reply.started": "2022-11-25T10:12:08.692513Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell #python의 대화형 쉘, 인터프리터\n",
    "InteractiveShell.ast_node_interactivity = \"all\"#모든 출력값을 연속적으로 출력\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:53:11.354542Z",
     "iopub.status.busy": "2022-11-25T10:53:11.353759Z",
     "iopub.status.idle": "2022-11-25T10:53:11.362427Z",
     "shell.execute_reply": "2022-11-25T10:53:11.361481Z",
     "shell.execute_reply.started": "2022-11-25T10:53:11.354487Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pyforest\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# import statistical libraries\n",
    "from scipy.stats import norm,skew, boxcox_normmax\n",
    "\n",
    "pd.options.display.max_rows = 1000000\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:12:14.621774Z",
     "iopub.status.busy": "2022-11-25T10:12:14.621151Z",
     "iopub.status.idle": "2022-11-25T10:12:22.746716Z",
     "shell.execute_reply": "2022-11-25T10:12:22.745511Z",
     "shell.execute_reply.started": "2022-11-25T10:12:14.621735Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/airbnb-new-york-city-with-106-features/airbnbmark1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11076\\2478735476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mabm1\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/airbnb-new-york-city-with-106-features/airbnbmark1.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mabm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'abm1.shape'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mabm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'abm1.size'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mabm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/airbnb-new-york-city-with-106-features/airbnbmark1.csv'"
     ]
    }
   ],
   "source": [
    "abm1 =  pd.read_csv('airbnbmark1.csv')\n",
    "abm1.head(3)\n",
    "print('abm1.shape',abm1.shape)\n",
    "print('abm1.size',abm1.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Steps Followed<center>\n",
    "    \n",
    "##   I. Data Processing\n",
    "##  II. EDA\n",
    "## III. Feature Engineering\n",
    "##  IV. Feature Selection\n",
    "##   V. Model Building  \n",
    "\n",
    " \n",
    " --------------------------------------------------------------------------------------------------------------------------  \n",
    "# I. Data Processing \n",
    " **We will do these following steps in Data Processing part:**\n",
    "\n",
    "- 1. Data Cleaning for special characters,spaces,nan and Checking Data types\n",
    "- 2. Extracting new features from existing features\n",
    "- 3. Imputing Missing Values\n",
    "- 4. Check Numerical, Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Dropping of Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:14:03.528216Z",
     "iopub.status.busy": "2022-11-25T10:14:03.527771Z",
     "iopub.status.idle": "2022-11-25T10:14:03.576847Z",
     "shell.execute_reply": "2022-11-25T10:14:03.575806Z",
     "shell.execute_reply.started": "2022-11-25T10:14:03.528158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping columns that are irrelevant to our analysis \n",
    "\n",
    "# Created New Variable abm2 for dataset after dropping columns\n",
    "\n",
    "abm = abm1.drop(columns = ['id','name',\n",
    "                'summary','access','interaction',\n",
    "                'listing_url','scrape_id','last_scraped',\n",
    "                'space','description','experiences_offered',\n",
    "                'neighborhood_overview','notes','transit',\n",
    "                'house_rules','thumbnail_url','medium_url',\n",
    "                'picture_url','xl_picture_url','host_url',\n",
    "                'host_name','host_location','host_about',\n",
    "                'host_acceptance_rate','host_thumbnail_url','host_picture_url',\n",
    "                'host_neighbourhood','host_verifications','host_has_profile_pic',\n",
    "                'market','city','smart_location','country_code','is_location_exact',\n",
    "                'square_feet','minimum_minimum_nights','maximum_minimum_nights',\n",
    "                'minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm',\n",
    "                'maximum_nights_avg_ntm','calendar_updated','zipcode',\n",
    "                'state',\n",
    "                'street','host_listings_count',#'neighbourhood',\n",
    "                'country','availability_30','availability_60','availability_90','host_id',\n",
    "                'calendar_last_scraped','weekly_price','monthly_price',\n",
    "                'review_scores_cleanliness','review_scores_checkin','review_scores_communication',\n",
    "                'review_scores_location','review_scores_value','license',\n",
    "                'jurisdiction_names','requires_license',\n",
    "                'is_business_travel_ready','require_guest_profile_picture','require_guest_phone_verification',\n",
    "                'calculated_host_listings_count','calculated_host_listings_count_entire_homes',\n",
    "                'calculated_host_listings_count_private_rooms',\n",
    "                'calculated_host_listings_count_shared_rooms','has_availability'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:14:11.309439Z",
     "iopub.status.busy": "2022-11-25T10:14:11.308949Z",
     "iopub.status.idle": "2022-11-25T10:14:11.315993Z",
     "shell.execute_reply": "2022-11-25T10:14:11.315013Z",
     "shell.execute_reply.started": "2022-11-25T10:14:11.309400Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1.shape # new dataset after removing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:14:25.746544Z",
     "iopub.status.busy": "2022-11-25T10:14:25.746099Z",
     "iopub.status.idle": "2022-11-25T10:14:26.319919Z",
     "shell.execute_reply": "2022-11-25T10:14:26.319039Z",
     "shell.execute_reply.started": "2022-11-25T10:14:25.746507Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1 = abm1.drop_duplicates() #중복된 데이터 행 삭제\n",
    "print('abm1.shape after dropping duplicate rows: ',abm1.shape)\n",
    "print('abm1.size:  ',abm1.size)\n",
    "print('DataTypes wise size: \\n', abm1.dtypes.value_counts())\n",
    "abm1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note1: We are left with 38 features after dropping initial columns which are repetative counts, irrelevant to anaysis, long text data, URL's and 1,22,818 rows after dropping duplicates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:14:55.287852Z",
     "iopub.status.busy": "2022-11-25T10:14:55.287443Z",
     "iopub.status.idle": "2022-11-25T10:14:55.494300Z",
     "shell.execute_reply": "2022-11-25T10:14:55.493201Z",
     "shell.execute_reply.started": "2022-11-25T10:14:55.287820Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1.replace(('11249\\n11249'),11249,inplace=True) #pandas.replace 해당 값을 다른 값으로 대체\n",
    "abm1.replace((' '),np.nan,inplace=True) #inplace가 T면 새로운 return 값이 아닌 원본을 아예 수정\n",
    "abm1.host_response_rate = abm1.host_response_rate.str[:-1].astype('float64') #%를 떼어내고 타입 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:15:00.791289Z",
     "iopub.status.busy": "2022-11-25T10:15:00.790806Z",
     "iopub.status.idle": "2022-11-25T10:15:01.443151Z",
     "shell.execute_reply": "2022-11-25T10:15:01.442003Z",
     "shell.execute_reply.started": "2022-11-25T10:15:00.791246Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(df): #$표시 떼어내기. ,구분표 없애기\n",
    "    \n",
    "    for i in ['price','cleaning_fee','security_deposit', 'extra_people']:\n",
    "        df[i]=df[i].str.replace('$','').str.replace(',', '').astype(float)\n",
    "        \n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    \n",
    "    return df.head(2)\n",
    "clean_data(abm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:15:44.929776Z",
     "iopub.status.busy": "2022-11-25T10:15:44.929347Z",
     "iopub.status.idle": "2022-11-25T10:15:44.935966Z",
     "shell.execute_reply": "2022-11-25T10:15:44.934750Z",
     "shell.execute_reply.started": "2022-11-25T10:15:44.929744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting Price our TARGET VAR into FLOAT\n",
    "abm1['price']=abm1['price'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:15:49.923499Z",
     "iopub.status.busy": "2022-11-25T10:15:49.923036Z",
     "iopub.status.idle": "2022-11-25T10:15:51.134756Z",
     "shell.execute_reply": "2022-11-25T10:15:51.133591Z",
     "shell.execute_reply.started": "2022-11-25T10:15:49.923454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing columns with f/t with 0/1\n",
    "abm1.replace({'f': 0, 't': 1}, inplace=True) \n",
    "\n",
    "#host_super host,instantbookable,identityverified,has_availabilty,requires licence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:16:30.776627Z",
     "iopub.status.busy": "2022-11-25T10:16:30.776145Z",
     "iopub.status.idle": "2022-11-25T10:16:30.923502Z",
     "shell.execute_reply": "2022-11-25T10:16:30.922424Z",
     "shell.execute_reply.started": "2022-11-25T10:16:30.776587Z"
    }
   },
   "outputs": [],
   "source": [
    "# converting Host_since dtype to datetime and creating new column host_days_active_years\n",
    "\n",
    "#의문점: 이 사람은 왜 active years 컬럼이 필요했을까?\n",
    "from datetime import datetime\n",
    "\n",
    "#날짜형식 오브젝트들을 datetime 형식으로 바꾼다.\n",
    "abm1.host_since = pd.to_datetime(abm1.host_since)\n",
    "abm1.first_review = pd.to_datetime(abm1.first_review)\n",
    "abm1.last_review = pd.to_datetime(abm1.last_review)\n",
    "\n",
    "# Calculating the number of years and days\n",
    "abm1['host_days_active_years'] = (datetime(2020, 4, 1) - abm1.host_since).astype('timedelta64[Y]')\n",
    "abm1['host_listing_since'] = (abm1.last_review - abm1.first_review).astype('timedelta64[Y]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:17:23.288255Z",
     "iopub.status.busy": "2022-11-25T10:17:23.287633Z",
     "iopub.status.idle": "2022-11-25T10:17:24.869098Z",
     "shell.execute_reply": "2022-11-25T10:17:24.868085Z",
     "shell.execute_reply.started": "2022-11-25T10:17:23.288214Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting amenities feature \n",
    "\n",
    "amenities_list = list(abm1.amenities)\n",
    "amenities_list_string = \" \".join(amenities_list)\n",
    "amenities_list_string = amenities_list_string.replace('{', '')\n",
    "amenities_list_string = amenities_list_string.replace('}', ',')\n",
    "amenities_list_string = amenities_list_string.replace('\"', '')\n",
    "amenities_set = [x.strip() for x in amenities_list_string.split(',')]\n",
    "amenities_set = set(amenities_set)\n",
    "print('\\n Number of amenities present in total:',len(amenities_set))\n",
    "print(amenities_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:22:42.936832Z",
     "iopub.status.busy": "2022-11-25T10:22:42.936377Z",
     "iopub.status.idle": "2022-11-25T10:22:50.989655Z",
     "shell.execute_reply": "2022-11-25T10:22:50.988484Z",
     "shell.execute_reply.started": "2022-11-25T10:22:42.936787Z"
    }
   },
   "outputs": [],
   "source": [
    "#직접... 연관된 amenities를 하나의 카테고리화 하여 묶었다.\n",
    "#그리고 각 amenities에 대해 새로운 컬럼을 추가하였다.\n",
    "#하지만 의문점은 중복된 단어 문구가 있을 경우, 엉뚱한 amenity로 설정될 수도 있다는 것이다.\n",
    "#loc로 행 조회후 컬럼값을 true인 경우 1로 바꿨다.\n",
    "abm1.loc[abm1['amenities'].str.contains('Air conditioning|Central air conditioning'), 'air_conditioning'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Amazon Echo|Apple TV|Game console|Netflix|Projector and screen|Smart TV'), 'high_end_electronics'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('BBQ grill|Fire pit|Propane barbeque'), 'bbq'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Balcony|Patio'), 'balcony'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Beach view|Beachfront|Lake access|Mountain view|Ski-in/Ski-out|Waterfront'), 'nature_and_views'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Bed linens'), 'bed_linen'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Breakfast'), 'breakfast'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('TV'), 'tv'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Coffee maker|Espresso machine'), 'coffee_machine'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Cooking basics'), 'cooking_basics'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Dishwasher|Dryer|Washer'), 'white_goods'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Elevator'), 'elevator'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Exercise equipment|Gym|gym'), 'gym'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Family/kid friendly|Children|children'), 'child_friendly'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('parking'), 'parking'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Garden|Outdoor|Sun loungers|Terrace'), 'outdoor_space'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Host greets you'), 'host_greeting'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Hot tub|Jetted tub|hot tub|Sauna|Pool|pool'), 'hot_tub_sauna_or_pool'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Internet|Pocket wifi|Wifi'), 'internet'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Long term stays allowed'), 'long_term_stays'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Pets|pet|Cat(s)|Dog(s)'), 'pets_allowed'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Private entrance'), 'private_entrance'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Safe|Security system'), 'secure'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Self check-in'), 'self_check_in'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Smoking allowed'), 'smoking_allowed'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Step-free access|Wheelchair|Accessible'), 'accessible'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('Suitable for events'), 'event_suitable'] = 1\n",
    "abm1.loc[abm1['amenities'].str.contains('24-hour check-in'), 'check_in_24h'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:22:50.992056Z",
     "iopub.status.busy": "2022-11-25T10:22:50.991746Z",
     "iopub.status.idle": "2022-11-25T10:22:51.021032Z",
     "shell.execute_reply": "2022-11-25T10:22:51.020258Z",
     "shell.execute_reply.started": "2022-11-25T10:22:50.992026Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:22:56.108477Z",
     "iopub.status.busy": "2022-11-25T10:22:56.107991Z",
     "iopub.status.idle": "2022-11-25T10:22:56.116104Z",
     "shell.execute_reply": "2022-11-25T10:22:56.115156Z",
     "shell.execute_reply.started": "2022-11-25T10:22:56.108430Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Amenities Column Names:\\n',abm1.columns[38:],'\\n')\n",
    "print(' Number of Amenities columns after categorizing under same names:',abm1.columns[38:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2:\n",
    "Above are the Columns after categorizing amenieties with similar names we are left with 28 features from 148 columns in amenities set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:23:58.876528Z",
     "iopub.status.busy": "2022-11-25T10:23:58.876070Z",
     "iopub.status.idle": "2022-11-25T10:23:58.919785Z",
     "shell.execute_reply": "2022-11-25T10:23:58.918746Z",
     "shell.execute_reply.started": "2022-11-25T10:23:58.876492Z"
    }
   },
   "outputs": [],
   "source": [
    "frequent_amenities = []\n",
    "infrequent_amenities=[]\n",
    "for col in abm1.iloc[:,38:].columns:\n",
    "    if abm1[col].sum() > len(abm1)/4: #전체에서 25%를 넘겼으면 common한 amenities\n",
    "        frequent_amenities.append(col)\n",
    "    else:\n",
    "        infrequent_amenities.append(col)\n",
    "print('Common_amenities: \\n',frequent_amenities)\n",
    "print('-----------------------')\n",
    "print('Special_amenities: \\n',infrequent_amenities)\n",
    "print('frequent_amenities',len(frequent_amenities))\n",
    "print('infrequent_amenities',len(infrequent_amenities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:24:23.135202Z",
     "iopub.status.busy": "2022-11-25T10:24:23.134797Z",
     "iopub.status.idle": "2022-11-25T10:24:23.182162Z",
     "shell.execute_reply": "2022-11-25T10:24:23.181118Z",
     "shell.execute_reply.started": "2022-11-25T10:24:23.135156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Decreasig the value_counts in cancellation policy \n",
    "abm1.cancellation_policy.replace({\n",
    "    'super_strict_30': 'strict',\n",
    "    'super_strict_60': 'strict',\n",
    "    'strict_14_with_grace_period': 'strict'}, inplace=True)\n",
    "abm1.cancellation_policy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:24:31.650269Z",
     "iopub.status.busy": "2022-11-25T10:24:31.649795Z",
     "iopub.status.idle": "2022-11-25T10:24:31.750311Z",
     "shell.execute_reply": "2022-11-25T10:24:31.749084Z",
     "shell.execute_reply.started": "2022-11-25T10:24:31.650230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Decreasig the value_counts in property_type\n",
    "# 열의 각 값들에 대한 발생횟수\n",
    "abm1['property_type'].value_counts()\n",
    "\n",
    "abm1['property_type'].value_counts()/abm1['property_type'].value_counts().sum()*100\n",
    "# 백분율\n",
    "\n",
    "#With 10 categories we account for 98% of the listings\n",
    "\n",
    "(abm1['property_type'].value_counts()/abm1['property_type'].value_counts().sum()*100)[0:5].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:25:00.392928Z",
     "iopub.status.busy": "2022-11-25T10:25:00.392515Z",
     "iopub.status.idle": "2022-11-25T10:25:00.432428Z",
     "shell.execute_reply": "2022-11-25T10:25:00.431016Z",
     "shell.execute_reply.started": "2022-11-25T10:25:00.392894Z"
    }
   },
   "outputs": [],
   "source": [
    "Mod_prop_type=abm1['property_type'].value_counts()[5:len(abm1['property_type'].value_counts())].index.tolist()\n",
    "\n",
    "def change_prop_type(label):\n",
    "    if label in Mod_prop_type:\n",
    "        label='Other'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:25:04.593608Z",
     "iopub.status.busy": "2022-11-25T10:25:04.593130Z",
     "iopub.status.idle": "2022-11-25T10:25:04.703771Z",
     "shell.execute_reply": "2022-11-25T10:25:04.702599Z",
     "shell.execute_reply.started": "2022-11-25T10:25:04.593564Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1.loc[:,'property_type'] = abm1.loc[:,'property_type'].apply(change_prop_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:25:07.407643Z",
     "iopub.status.busy": "2022-11-25T10:25:07.407253Z",
     "iopub.status.idle": "2022-11-25T10:25:07.435547Z",
     "shell.execute_reply": "2022-11-25T10:25:07.434713Z",
     "shell.execute_reply.started": "2022-11-25T10:25:07.407611Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1['property_type'].value_counts() # 5순위 이하 주거형태는 Others로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note3: We are going to create new column that sums up the total number of ameniteis present by each host**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:26:18.584478Z",
     "iopub.status.busy": "2022-11-25T10:26:18.584008Z",
     "iopub.status.idle": "2022-11-25T10:26:18.644891Z",
     "shell.execute_reply": "2022-11-25T10:26:18.643773Z",
     "shell.execute_reply.started": "2022-11-25T10:26:18.584438Z"
    }
   },
   "outputs": [],
   "source": [
    "#각 숙소마다 special한 amenities를 몇개씩 가지고 있는지\n",
    "abm1['special_amenities']=abm1[['high_end_electronics', 'bbq', 'balcony',\n",
    "                                'nature_and_views', 'breakfast', 'gym',\n",
    "                                'child_friendly', 'outdoor_space', 'host_greeting',\n",
    "                                'hot_tub_sauna_or_pool', 'long_term_stays', 'pets_allowed',\n",
    "                                'private_entrance', 'secure', 'self_check_in',\n",
    "                                'smoking_allowed', 'accessible', 'event_suitable',\n",
    "                                'check_in_24h']].sum(axis=1)\n",
    "abm1['special_amenities'].isnull().sum()\n",
    "abm1.columns\n",
    "abm1['special_amenities'].astype(float)\n",
    "abm1['special_amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:26:26.887370Z",
     "iopub.status.busy": "2022-11-25T10:26:26.886917Z",
     "iopub.status.idle": "2022-11-25T10:26:26.990711Z",
     "shell.execute_reply": "2022-11-25T10:26:26.989518Z",
     "shell.execute_reply.started": "2022-11-25T10:26:26.887332Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1.isnull().sum() #결측값 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:27:18.427094Z",
     "iopub.status.busy": "2022-11-25T10:27:18.426666Z",
     "iopub.status.idle": "2022-11-25T10:27:18.534752Z",
     "shell.execute_reply": "2022-11-25T10:27:18.533606Z",
     "shell.execute_reply.started": "2022-11-25T10:27:18.427059Z"
    }
   },
   "outputs": [],
   "source": [
    "## code for merging amenities into special features and if one amenity is present the value is 1 else 0\n",
    "abm1['common_amenities']=abm1[['air_conditioning', 'bed_linen', 'tv',\n",
    "                               'coffee_machine', 'cooking_basics', 'white_goods',\n",
    "                               'elevator', 'parking', 'internet']].sum(axis=1)\n",
    "abm1['common_amenities'].isnull().sum()\n",
    "abm1.columns\n",
    "abm1['common_amenities'].astype(float)\n",
    "# abm1['common_amenities']=abm1['common_amenities'].mask(abm1['common_amenities']>0,1)\n",
    "abm1['common_amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T10:56:12.521387Z",
     "iopub.status.busy": "2022-11-25T10:56:12.520967Z",
     "iopub.status.idle": "2022-11-25T10:56:12.910458Z",
     "shell.execute_reply": "2022-11-25T10:56:12.909486Z",
     "shell.execute_reply.started": "2022-11-25T10:56:12.521356Z"
    }
   },
   "outputs": [],
   "source": [
    "abm1.columns[38:-2]\n",
    "abm1 = abm1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T11:06:42.033407Z",
     "iopub.status.busy": "2022-11-25T11:06:42.032930Z",
     "iopub.status.idle": "2022-11-25T11:06:42.039993Z",
     "shell.execute_reply": "2022-11-25T11:06:42.038851Z",
     "shell.execute_reply.started": "2022-11-25T11:06:42.033366Z"
    }
   },
   "outputs": [],
   "source": [
    "amenities_col = ['air_conditioning', 'high_end_electronics', 'bbq', 'balcony',\n",
    "       'nature_and_views', 'bed_linen', 'breakfast', 'tv', 'coffee_machine',\n",
    "       'cooking_basics', 'white_goods', 'elevator', 'gym', 'child_friendly',\n",
    "       'parking', 'outdoor_space', 'host_greeting', 'hot_tub_sauna_or_pool',\n",
    "       'internet', 'long_term_stays', 'pets_allowed', 'private_entrance',\n",
    "       'secure', 'self_check_in', 'smoking_allowed', 'accessible',\n",
    "       'event_suitable', 'check_in_24h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T11:22:19.852374Z",
     "iopub.status.busy": "2022-11-25T11:22:19.851804Z",
     "iopub.status.idle": "2022-11-25T11:22:20.071305Z",
     "shell.execute_reply": "2022-11-25T11:22:20.070514Z",
     "shell.execute_reply.started": "2022-11-25T11:22:19.852336Z"
    }
   },
   "outputs": [],
   "source": [
    "amenities_weight = {}\n",
    "\n",
    "for col in amenities_col:\n",
    "    weight = abm1.groupby([col,])['review_scores_rating'].mean()[1]-abm1.groupby([col])['review_scores_rating'].mean()[0]\n",
    "    amenities_weight[col] = weight\n",
    "    \n",
    "print(sorted(amenities_weight.items(), key = lambda item: item[1], reverse = True))\n",
    "# write.csv(new_col, \"spc_am_rating.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> new name is given to the dataframe \"abm2\" after adding new cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T09:47:31.924374Z",
     "iopub.status.busy": "2022-11-25T09:47:31.923911Z",
     "iopub.status.idle": "2022-11-25T09:47:32.131967Z",
     "shell.execute_reply": "2022-11-25T09:47:32.131160Z",
     "shell.execute_reply.started": "2022-11-25T09:47:31.924333Z"
    }
   },
   "outputs": [],
   "source": [
    "abm2 = abm1.merge(new_col,left_on=['neighbourhood_cleansed','property_type'],right_on=['neighbourhood_cleansed','property_type'],how='left')\n",
    "print(abm2.shape)\n",
    "print(abm2.size)\n",
    "abm2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> new name is given to the dataframe \"abm3\" after adding new cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T09:52:05.090369Z",
     "iopub.status.busy": "2022-11-25T09:52:05.089902Z",
     "iopub.status.idle": "2022-11-25T09:52:05.128745Z",
     "shell.execute_reply": "2022-11-25T09:52:05.127404Z",
     "shell.execute_reply.started": "2022-11-25T09:52:05.090328Z"
    }
   },
   "outputs": [],
   "source": [
    "new_col1 = pd.DataFrame(columns=['avg_review_score'])\n",
    "new_col1['avg_review_score'] = abm2.groupby(['neighbourhood_cleansed','property_type'])['review_scores_rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3 = abm2.merge(new_col1,left_on=['neighbourhood_cleansed','property_type'],right_on=['neighbourhood_cleansed','property_type'],how='left')\n",
    "print(abm3.shape)\n",
    "print(abm3.size)\n",
    "abm3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbourhood_group_cleansed is renamed as Borough\n",
    "#abm3['Borough'] = abm3['neighbourhood_group_cleansed']\n",
    "\n",
    "# guests_included is renamed as Num_of_guests_incl_forprice\n",
    "#abm3['Num_of_guests_incl_forprice'] = abm3['guests_included']\n",
    "\n",
    "# extra_people is renamed as price_per_extra_people\n",
    "#abm3['price_per_extra_people'] = abm3['extra_people']\n",
    "\n",
    "abm3=abm3.rename(columns={\"neighbourhood_group_cleansed\": \"Borough\", \"guests_included\": \"Num_of_guests_incl_forprice\"\n",
    "                    ,'extra_people':'price_per_extra_people'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPUTING MISSING NAN VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(missing_values=np.nan,n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "abm3['host_total_listings_count'] = imputer.fit_transform(abm3[['host_total_listings_count']])\n",
    "#abm1['host_days_active_years'] = imputer.fit_transform(abm1[['host_days_active_years']])\n",
    "#abm1['host_days_active_days'] = imputer.fit_transform(abm1[['host_days_active_days']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by neighborhood_cleansed and property_type fill in missing value by the median.\n",
    "\n",
    "# abm3[\"security_deposit\"] = abm3.groupby(['neighbourhood_cleansed','property_type'])[\"security_deposit\"].transform(\n",
    "#     lambda x: x.fillna(x.median()))\n",
    "\n",
    "# abm3[\"cleaning_fee\"] = abm3.groupby(['neighbourhood_cleansed','property_type'])[\"cleaning_fee\"].transform(\n",
    "#     lambda x: x.fillna(x.median()))\n",
    "\n",
    "abm3[\"beds\"] = abm3.groupby(['neighbourhood_cleansed','property_type'])[\"beds\"].transform(\n",
    "    lambda x: x.fillna(x.mode()))\n",
    "\n",
    "abm3[\"bathrooms\"] = abm3.groupby(['neighbourhood_cleansed','property_type'])[\"bathrooms\"].transform(\n",
    "    lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "abm3[\"bedrooms\"] = abm3.groupby(['neighbourhood_cleansed','property_type'])[\"bedrooms\"].transform(\n",
    "    lambda x: x.fillna(x.mode()))\n",
    "\n",
    "abm3[\"review_scores_rating\"] = abm3.groupby(['neighbourhood_cleansed','property_type'])[\"review_scores_rating\"].transform(\n",
    "    lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# abm3[\"review_scores_accuracy\"] = abm3.groupby(['neighbourhood_cleansed','property_type'])[\"review_scores_accuracy\"].transform(\n",
    "#     lambda x: x.fillna(x.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by neighborhood_cleansed and property_type fill in missing value by the mode.\n",
    "abm3[\"host_listing_since\"] = abm3.groupby(['neighbourhood_cleansed'])[\"host_listing_since\"].transform(\n",
    "    lambda x: x.fillna(x.mode()))\n",
    "\n",
    "\n",
    "abm3[\"host_is_superhost\"] = abm3.groupby(['neighbourhood_cleansed'])[\"host_is_superhost\"].transform(\n",
    "    lambda x: x.fillna(x.mode()))\n",
    "\n",
    "\n",
    "abm3[\"host_identity_verified\"] = abm3.groupby(['neighbourhood_cleansed'])[\"host_identity_verified\"].transform(\n",
    "    lambda x: x.fillna(x.mode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nan_remove=['host_response_time']\n",
    "for i in features_nan_remove:\n",
    "    abm3[i]=abm3[i].astype('str').str.replace(\"nan\", \"unknown\").astype(str)\n",
    "    print('{}:{}'.format(i,abm2[i].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nan_remove=['review_scores_rating','host_response_rate','review_scores_accuracy','cleaning_fee','bedrooms','security_deposit','host_identity_verified','beds','host_days_active_years','review_scores_rating','host_is_superhost','host_listing_since','avg_review_score']\n",
    "for i in features_nan_remove:\n",
    "    abm3[i]=abm3[i].astype('str').str.replace(\"nan\", \"100000000\").astype('float')\n",
    "    print('{}:{}'.format(i,abm3[i].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nan_remove=['review_scores_rating','host_response_rate','review_scores_accuracy','cleaning_fee','bedrooms','security_deposit','host_identity_verified','beds','host_days_active_years','review_scores_rating','host_is_superhost','host_listing_since','avg_review_score']\n",
    "for i in features_nan_remove:\n",
    "    abm3[i]=abm3[i].replace(100000000, abm3[i].median())\n",
    "    print('{}:{}'.format(i,abm3[i].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating categorical and numerical dtypes\n",
    "categorical_types=abm3.select_dtypes(include=['object']).columns\n",
    "print('categorical_types: \\n',categorical_types)\n",
    "\n",
    "print('-------------')\n",
    "\n",
    "numerical_types=abm3._get_numeric_data().columns\n",
    "print('numerical_types: \\n',numerical_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  II. EDA\n",
    "- Exploring the Data.\n",
    "- Getting Business Insights from the data\n",
    "- Cardinality of Categorical features(PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical varibles further breaking down \n",
    "# CONTINOUS AND DISCRETE VARIABLES\n",
    "\n",
    "Discrete_features=[i for i in numerical_types if len(abm3[i].unique())<25]\n",
    "print(Discrete_features)\n",
    "print(len(Discrete_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relationship between discrete var and price\n",
    "for i in Discrete_features:\n",
    "    abm3.groupby(i)['price'].mean().plot.bar()\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel('price')\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to be taken from above:\n",
    "- from the above we can see the relationship being a superhost or not doesnot affect price\n",
    "- accomodates has a linear increase in price with increase in accomadates\n",
    "- special features also does not affect price much until special amenities count is greater than 7\n",
    "- common amenities present properties have same price regardless of neighbourhood.\n",
    "- review scores does not effect price of the property. So, we can drop review_score_accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets compare the difference between years and price\n",
    "plt.scatter(abm3.host_days_active_years,abm3.price)\n",
    "plt.title('Host_active_years Vs Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note6:We can see that aas the number of years a host is registered and active the price is decreasing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Continuous var\n",
    "Continous_features=[i for i in numerical_types if i not in Discrete_features]\n",
    "Continous_features[1:] #since Host_id is not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting continous var\n",
    "for i in Continous_features[1:]:\n",
    "    data=abm3.copy()\n",
    "    if 0 in data[i].unique():\n",
    "        pass\n",
    "    elif i in ['zipcode','latitude','longitude']:\n",
    "        pass\n",
    "    else:\n",
    "#         data[i]=np.log(data[i])\n",
    "        data[i].hist(bins=25)\n",
    "        plt.xlabel(i)\n",
    "        plt.ylabel('count')\n",
    "        plt.title(i)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: From the above plots we observe the data is either right positively or negatively skewed, and also not normally distributed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Avg age of listings group by borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abm3.groupby(['Borough'])['host_days_active_years'].mean().sort_values())\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.countplot(x ='host_days_active_years',hue = \"Borough\",data = abm3)\n",
    "plt.title(\"Hosting since across boroughs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brooklyn and Manhattan have the oldest listings by nearly 4 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -price distribution of various room types across neighbourhood groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is clearly seen that hotel rooms are much costlier than entire house in manhattan and brooklyn region \n",
    "- overall entire apt/home price is higher than private room\n",
    "- Also it is seen that staten island and bronx do not have any hotel rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abm3.groupby(['room_type','Borough'])['price'].mean().sort_values())\n",
    "sns.set(rc={'figure.figsize': (16, 5)})\n",
    "ax = sns.barplot(x = 'Borough', y = 'price', hue = 'room_type', data =abm3, \n",
    "                 palette ='plasma_r', ci = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly reviewed of every feature based on Number of reviews(last twelve months)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In entire city we have more number of Appartment listings\n",
    "- Except for brooklyn every Borough has more Appartments than other property types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abm3.groupby(['property_type'])['number_of_reviews_ltm'].mean())\n",
    "sns.set(rc={'figure.figsize': (16, 5)})\n",
    "ax = sns.barplot(x = 'Borough', y = 'number_of_reviews_ltm', hue = 'property_type', data =abm3, \n",
    "                 palette ='plasma_r', ci = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abm3.groupby(['room_type'])['number_of_reviews_ltm'].mean())\n",
    "sns.set(rc={'figure.figsize': (16, 5)})\n",
    "ax = sns.barplot(x = 'Borough', y = 'number_of_reviews_ltm', hue = 'room_type', data =abm3, \n",
    "                 palette ='plasma_r', ci = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.graph_objs as go\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from plotly import tools\n",
    "\n",
    "# fig = px.scatter_mapbox(abm3, \n",
    "#                         hover_data = ['price','minimum_nights','room_type'],\n",
    "#                         hover_name = 'neighbourhood_cleansed',\n",
    "#                         lat=\"latitude\", \n",
    "#                         lon=\"longitude\", \n",
    "#                         color=\"Borough\", \n",
    "#                         size=\"price\", \n",
    "#                         size_max=30, \n",
    "#                         opacity = .70,\n",
    "#                         zoom=10,\n",
    "#                        )\n",
    "# fig.layout.mapbox.style = 'stamen-terrain'\n",
    "# fig.update_layout(title_text = 'Airbnb by Borough in NYC<br>(Click legend to toggle borough)', height = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter_mapbox(abm3,\n",
    "#                         hover_data=['price','property_type','room_type','number_of_reviews_ltm'], \n",
    "#                         lat=\"latitude\", \n",
    "#                         lon=\"longitude\", \n",
    "#                         color=\"neighbourhood_cleansed\", \n",
    "#                         size_max=30, \n",
    "#                         opacity = .70,\n",
    "#                         zoom=12,\n",
    "#                        )\n",
    "# fig.layout.mapbox.style = 'carto-positron'\n",
    "# fig.update_layout(title_text = 'NYC Airbnb by Neighbourhood<br>(Click legend to toggle neighbourhood)', height = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_bk = abm3[abm3.Borough == 'Brooklyn']\n",
    "# temp_qn = abm3[abm3.Borough == 'Queens']\n",
    "# temp_mn = abm3[abm3.Borough == 'Manhattan']\n",
    "# temp_bx = abm3[abm3.Borough == 'Bronx']\n",
    "# temp_si = abm3[abm3.Borough == 'Staten Island']\n",
    "\n",
    "# labels = abm3.room_type.value_counts().index.to_list()\n",
    "\n",
    "# fig = make_subplots(1, 5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'},{'type':'domain'},{'type':'domain'}]],\n",
    "#                     subplot_titles=['Manhattan', 'Brooklyn', 'Queens','Bronx','Staten Island'])\n",
    "# fig1= fig.add_trace(go.Pie(labels=labels, values=temp_mn.room_type.value_counts().reset_index().sort_values(by = 'index').room_type.tolist(), scalegroup='one',\n",
    "#                      name=\"Manhattan\"),1,1)\n",
    "# fig2= fig.add_trace(go.Pie(labels=labels, values=temp_bk.room_type.value_counts().reset_index().sort_values(by = 'index').room_type.tolist(), scalegroup='one',\n",
    "#                      name=\"Brooklyn\"),1,2)\n",
    "# fig3= fig.add_trace(go.Pie(labels=labels, values=temp_qn.room_type.value_counts().reset_index().sort_values(by = 'index').room_type.tolist(), scalegroup='one',\n",
    "#                      name=\"Queens\"),1,3)\n",
    "# fig4= fig.add_trace(go.Pie(labels=labels, values=temp_bx.room_type.value_counts().reset_index().sort_values(by = 'index').room_type.tolist(), scalegroup='one',\n",
    "#                      name=\"Bronx\"),1,4)\n",
    "# fig5= fig.add_trace(go.Pie(labels=labels, values=temp_si.room_type.value_counts().reset_index().sort_values(by = 'index').room_type.tolist(), scalegroup='one',\n",
    "#                      name=\"Staten Island\"),1,5)\n",
    "\n",
    "# fig.update_layout(title_text='room types in Boroughs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Cardinality of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in categorical_types:\n",
    "        print('feature-{} & number of categories-{}'.format(i,len(abm3[i].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relationship b/w categorical and dependent var\n",
    "for i in categorical_types:\n",
    "    if len(data[i].unique())>40:\n",
    "        pass\n",
    "    elif len(data[i].unique())==1:\n",
    "        pass\n",
    "    else:\n",
    "        data.groupby(i)['price'].mean().plot.bar()\n",
    "        plt.xlabel(i)\n",
    "        plt.ylabel('price')\n",
    "        plt.title(i)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from the above we can observe relationship between categorical feature and price feature and also we can reduce the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Objective: To perform Statistical Analysis on the data set by implementing various stats modules (on New York AirBnb data) such as Hypothesis Testing, Tests of Mean (Kruskal Wallis Test, ANOVA - one way and two way), Tests of Proportion (z test and chi-squared test) and Tests of Variance (F-test, Levene test), after checking for the three assumptions of (i) Normality of target variable (ii) Randomness of Sampling (iii) Equal variance across categories. The level of significance is assumed to be 5 percent (i.e. alpha = 0.05) If assumptions are satisfied, parametric tests(assumes already distribution is present(ANOVA) can be performed, else non-parametric tests(do not rely on any distributions(CHI-SQUARE) have to be performed. The results of the tests performed will enable us to find the associativity and dependability of different features on one-another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = pd.crosstab(abm3.Borough,abm3.room_type)\n",
    "cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the assumptions:\n",
    "- Randomness of Data\n",
    "- Normality Test\n",
    "- Variance Test\n",
    "\n",
    "The target variable being the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(abm2.price,color='r')\n",
    "plt.xlabel(\"Price\")\n",
    "plt.title(\"Distribution of Price of among property types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapiro Test (for checking Normality)\n",
    "\n",
    "- H0 (Null Hypothesis) : Distribution is normal\n",
    "\n",
    "- H1 (Alternate Hypothesis): Distribution is not normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "st.shapiro(abm2.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levene Test (for testing of variance)\n",
    "H0 (null hypothesis): variance(private_room) = variance(shared_room) = variance(entire_home)=variance(hotel_room)\n",
    "\n",
    "H1 (alternate hypothesis): variance(private_room) != variance(shared_room) != variance(entire_home)!=variance(hotel_room)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvt = abm3[abm3['room_type'] == 'Private room']\n",
    "share = abm3[abm3['room_type'] == 'Shared room']\n",
    "apt = abm3[abm3['room_type'] == 'Entire home/apt']\n",
    "hotel=abm3[abm3['room_type'] == 'Hotel room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.levene(pvt.price, share.price, apt.price,hotel.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here P-value is less than 0.5 and therefore we can reject null hypothesis and can say that price varies in different room types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### price vs neighbourhood\n",
    "H0 (null hypothesis): mean_price(Brooklyn) = mean_price(Manhattan) = ..... = mean_price(Bronx)\n",
    "\n",
    "H1 (null hypothesis): mean_price(Brooklyn) != mean_price(Manhattan) != ..... != mean_price(Bronx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one way anova\n",
    "mod = ols('price ~ Borough', data =abm3).fit()\n",
    "aov_table = sm.stats.anova_lm(mod, typ=1)\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here pvalue obtained is greater than 0.5, so we fail to reject null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Room Type vs Neighbourhood Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both the variables Room Type and Neighbourhood Group are categorical having more than two categories, we can peform Chi-squared test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi Squared Test\n",
    "- H0 (null hypothesis): There is no association between Room Type and Neighbourhood Group.\n",
    "- H1 (alternate hypothesis): There is an association between Room Type and Neighbourhood Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.crosstab(abm3['room_type'],abm3['Borough'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.chi2_contingency(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(abm3['room_type'],abm3['Borough'])\n",
    "ct.plot.bar(stacked=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price vs review scores rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ols('price ~ review_scores_rating', data =abm3).fit()\n",
    "aov_table = sm.stats.anova_lm(mod, typ=1)\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here Pvalue obtained is less than 0.5, so we can reject null hypothesis and can say that reviews have an effect on price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price vs cancellation policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ols('price ~ cancellation_policy', data =abm3).fit()\n",
    "aov_table = sm.stats.anova_lm(mod, typ=1)\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here Pvalue obtained is less than 0.5, so we can reject null hypothesis and can say that cancellation policy have an effect on price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price vs availability_365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ols('price ~ availability_365', data =abm3).fit()\n",
    "aov_table = sm.stats.anova_lm(mod, typ=1)\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chi2 on response rate and property type, borough etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.crosstab(abm3['host_response_rate'],abm3['property_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.chi2_contingency(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.crosstab(abm3['host_response_rate'],abm3['Borough'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st.chi2_contingency(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical=[i for i in abm3.columns if abm3[i].dtypes!='O']\n",
    "len(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = abm3.corr()\n",
    "\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"price\"])\n",
    "\n",
    "# Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.0]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr_features = cor.index[abs(cor[\"price\"])>0.0]\n",
    "# plt.figure(figsize=(10,15))\n",
    "sns.heatmap(abm3[top_corr_features].corr(), annot = True, \n",
    "                cbar = True,square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we can see except avg_price_property_type is the only feature with above 0.3 correlation we are dropping none features and moving with feature Engineering and Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition check for Multicollinearity  between 2 newly extracted features\n",
    "print(abm3[[\"avg_review_score\",\"avg_price_property_type\"]].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Feature Engineering\n",
    "\n",
    "- Reducing Labels in the Object features\n",
    "- Handling Outliers\n",
    "- Transforming data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking objects in Categorical variable.\n",
    "print('host_response_time: \\n',abm3['host_response_time'].value_counts()/abm3['host_response_time'].value_counts().sum()*100)\n",
    "print('=================================================')\n",
    "print('cancellation_policy: \\n',abm3.cancellation_policy.value_counts()/abm3['cancellation_policy'].value_counts().sum()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducig labdels in Host_response_time.\n",
    "abm3.replace({'within an hour':'Hour','within a few hours':'One Day','within a day':'Days'},inplace=True)\n",
    "\n",
    "# Decreasig the value_counts in property_type\n",
    "\n",
    "#With 10 categories we account for 95% of the listings\n",
    "(abm3['property_type'].value_counts()/abm3['property_type'].value_counts().sum()*100)[0:5].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mod_prop_type=abm3['property_type'].value_counts()[5:len(abm1['property_type'].value_counts())].index.tolist()\n",
    "\n",
    "def change_prop_type(label):\n",
    "    if label in Mod_prop_type:\n",
    "        label='Other'\n",
    "    return label\n",
    "# Mod_prop_type\n",
    "abm3.loc[:,'property_type'] = abm3.loc[:,'property_type'].apply(change_prop_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abm3.property_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df = abm3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_plot(feat,df):\n",
    "    plt.rcParams['figure.figsize']=(15,15)\n",
    "    plt.style.use(style='ggplot')\n",
    "    xxx,sub=plt.subplots(5,6)\n",
    "    xxx.subplots_adjust(hspace=0.5)\n",
    "    sub=sub.flatten()\n",
    "    for i in range(len(feat)):\n",
    "        sub[i].scatter(x=df[feat[i]], y=np.log1p(abm3[\"price\"]),s=4)\n",
    "        sub[i].set_title('{}'.format(feat[i],fontsize=10))\n",
    "        sub[i].set_ylabel('log(Price)',fontsize=10)\n",
    "        sub[i].tick_params(labelsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "Numerical_cols=[i for i in cap_df.columns if cap_df[i].dtypes=='float64']\n",
    "len(Numerical_cols)\n",
    "Numerical_cols = list(Numerical_cols)\n",
    "# Numerical_cols.remove('price')\n",
    "features_plot(sorted(Numerical_cols),cap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_data(df):\n",
    "    for col in df.columns:\n",
    "#         print(\"capping the \",col)\n",
    "        if (((df[col].dtype)=='float64') | ((df[col].dtype)=='int64')):\n",
    "            percentiles = df[col].quantile([0.25,0.75]).values\n",
    "            df[col][df[col] <= percentiles[0]] = percentiles[0]\n",
    "            df[col][df[col] >= percentiles[1]] = percentiles[1]\n",
    "            print(percentiles)\n",
    "        else:\n",
    "            df[col]=df[col]\n",
    "    return df\n",
    "\n",
    "# abm3 = cap_data(abm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping outliers at lower and upper viscor\n",
    "cap_df = cap_data(cap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_plot(feat,df):\n",
    "    plt.rcParams['figure.figsize']=(15,15)\n",
    "    plt.style.use(style='ggplot')\n",
    "    xxx,sub=plt.subplots(5,6)\n",
    "    xxx.subplots_adjust(hspace=0.5)\n",
    "    sub=sub.flatten()\n",
    "    for i in range(len(feat)):\n",
    "        sub[i].scatter(x=df[feat[i]], y=np.log1p(df[\"price\"]),s=4)\n",
    "        sub[i].set_title('{}'.format(feat[i],fontsize=10))\n",
    "        sub[i].set_ylabel('log(Price)',fontsize=10)\n",
    "        sub[i].tick_params(labelsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "Numerical_cols=[i for i in cap_df.columns if cap_df[i].dtypes=='float64']\n",
    "len(Numerical_cols)\n",
    "Numerical_cols = list(Numerical_cols)\n",
    "# Numerical_cols.remove('price')\n",
    "features_plot(sorted(Numerical_cols),cap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = ['host_is_superhost', 'host_total_listings_count',\n",
    "       'host_identity_verified', 'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds', 'price']\n",
    "num_columns1 = ['price', 'availability_365','number_of_reviews_ltm', 'review_scores_rating','review_scores_accuracy', \n",
    "               'instant_bookable', 'host_days_active_years','host_listing_since']\n",
    "\n",
    "num_columns2 = ['special_amenities', \n",
    "               'common_amenities', 'avg_price_property_type', 'avg_review_score','price', 'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people','minimum_nights', 'maximum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=cap_df[num_columns],height=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=cap_df[num_columns1],height=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=cap_df[num_columns2],height=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = cap_df[numerical_types].drop(['latitude','longitude'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the skew of all numerical features\n",
    "skewed_feats = final_df.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "import numpy as np \n",
    "from scipy import stats \n",
    "\n",
    "# plotting modules \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# generate non-normal data (exponential) \n",
    "final_df = np.random.exponential(size = 1000) \n",
    "\n",
    "# transform training data & save lambda value \n",
    "fitted_data, fitted_lambda = stats.boxcox(final_df) \n",
    "\n",
    "# creating axes to draw plots \n",
    "fig, ax = plt.subplots(1, 2) \n",
    "\n",
    "# plotting the original data(non-normal) and \n",
    "# fitted data (normal) \n",
    "sns.distplot(final_df, hist = False, kde = True, \n",
    "\tkde_kws = {'shade': True, 'linewidth': 2}, \n",
    "\tlabel = \"Non-Normal\", color =\"green\", ax = ax[0]) \n",
    "\n",
    "sns.distplot(fitted_data, hist = False, kde = True, \n",
    "\tkde_kws = {'shade': True, 'linewidth': 2}, \n",
    "\tlabel = \"Normal\", color =\"green\", ax = ax[1]) \n",
    "\n",
    "# adding legends to the subplots \n",
    "plt.legend(loc = \"upper right\") \n",
    "\n",
    "# rescaling the subplots \n",
    "fig.set_figheight(5) \n",
    "fig.set_figwidth(10) \n",
    "\n",
    "print(f\"Lambda value used for Transformation: {fitted_lambda}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the skew of all numerical features\n",
    "skewed_feats = pd.DataFrame(fitted_data).apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Bin into 5 categories\n",
    "cap_df['host_response_rate'].value_counts(bins=5,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin into five categories\n",
    "cap_df['host_response_rate_bins'] = pd.cut(cap_df.host_response_rate, bins=[50,60,70,80,100], labels=['49-60%', '50-89%', '90-99%', '100%'], include_lowest=True)\n",
    "\n",
    "# Converting to string\n",
    "cap_df['host_response_rate_bins'] = cap_df['host_response_rate_bins'].astype('str')\n",
    "\n",
    "# Replace nulls with 'unknown'\n",
    "#cap_df['host_response_rate_bins'].replace('nan', 'unknown', inplace=True)\n",
    "\n",
    "# Category counts\n",
    "cap_df['host_response_rate_bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = pd.DataFrame(cap_df,columns = ['property_type','room_type','cancellation_policy','host_response_rate_bins','bed_type'])\n",
    "dfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = dfe.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    freqs = dfe[col].value_counts()\n",
    "    k = freqs.index[freqs>20][:6]\n",
    "    for cat in k:\n",
    "        name = col+'_'+cat\n",
    "        dfe[name]=(dfe[col]==cat).astype(int)\n",
    "    del dfe[col]\n",
    "    print(col)\n",
    "    \n",
    "print(dfe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abmen = pd.concat((dfe,cap_df),axis=1)\n",
    "abmen.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "final_df = abmen.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['host_response_time', 'host_response_rate','host_response_rate_bins', 'neighbourhood_cleansed',\n",
    "       'Borough', 'property_type', 'room_type', 'bed_type',\n",
    "       'cancellation_policy'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating categorical and numerical dtypes\n",
    "categorical_cols=final_df.select_dtypes(include=['object']).columns\n",
    "print('categorical_types: \\n',categorical_cols)\n",
    "\n",
    "print('-------------')\n",
    "\n",
    "numerical_cols=final_df._get_numeric_data().columns\n",
    "print('numerical_types: \\n',numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['longitude','latitude'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  IV. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Data Ready\n",
    "X = final_df.drop('price', axis=1)\n",
    "y= final_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing RFE model\n",
    "rfe = RFE(model, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['bedrooms', 'review_scores_accuracy'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of features\n",
    "nof_list=np.arange(1,47)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Raw linear regression model\n",
    "X = final_df.drop('price', axis=1)\n",
    "y= final_df['price']\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(f'Coefficients: {lin_reg.coef_}')\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'R^2 score: {lin_reg.score(X, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "model = lin_reg.fit(X_train,y_train)\n",
    "print(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\n",
    "print(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Raw OLS Model\n",
    "X = final_df.drop(['price'],axis=1)\n",
    "y = final_df.price\n",
    "X_constant = sm.add_constant(X)\n",
    "lin_reg = sm.OLS(y,X_constant).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Assumption 1- No autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "acf = smt.graphics.plot_acf(lin_reg.resid, lags=40 , alpha=0.05)\n",
    "acf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Assumption 2- Normality of Residuals\n",
    "from scipy import stats\n",
    "print(stats.jarque_bera(lin_reg.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(lin_reg.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Asssumption 3 - Linearity of residuals\n",
    "Here we have 2 options. Either we can plot the observed values Vs predicted values and plot the Residual Vs predicted values and see the linearity of residuals.\n",
    "OR\n",
    "We can go for rainbow test. Let's look both of them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rainbow test \n",
    "It is done to check the linearity of the residuals for a linear regression model.\n",
    "Linearity of residuals is preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.stats.diagnostic.linear_rainbow(res=lin_reg, frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pylab\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "st_residual = lin_reg.get_influence().resid_studentized_internal\n",
    "stats.probplot(st_residual, dist=\"norm\", plot = pylab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.resid.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very close to 0 so linearity is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Assumption 4 -  Homoscedasticity_test(using goldfeld test) OR (Beusch-Wagon Test)\n",
    "\n",
    "# goldfeld test\n",
    "from statsmodels.compat import lzip\n",
    "import numpy as np\n",
    "from statsmodels.compat import lzip\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "model = lin_reg\n",
    "fitted_vals = model.predict()\n",
    "resids = model.resid\n",
    "resids_standardized = model.get_influence().resid_studentized_internal\n",
    "\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model.resid, model.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Assumption 4 -  Homoscedasticity_test(using goldfeld test) OR (Beusch-Wagon Test)\n",
    "\n",
    "##### breuschpagan Test\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.compat import lzip\n",
    "name = ['Lagrange multiplier statistic', 'p-value',\n",
    "        'f-value', 'f p-value']\n",
    "test = sms.het_breuschpagan(lin_reg.resid, lin_reg.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so from above p- value we know that the data is heteroscedastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assumption 5- NO  MULTI COLLINEARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Assumption 5- NO  MULTI COLLINEARITY\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "pd.DataFrame({'vif': vif[0:]}, index=X.columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#So, multicollinearity exists.\n",
    "Note : This vif column has be built with the help of X_constant and not the X_values. Because we built our model by adding Constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(pd.DataFrame({'vif': vif[0:]}, index=X.columns).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  removed like correlated variables\n",
    "X = final_df[['property_type_Apartment', 'property_type_House', 'property_type_Other',\n",
    "       'property_type_Townhouse', 'property_type_Condominium',\n",
    "       'room_type_Entire home/apt', 'room_type_Private room',\n",
    "       'room_type_Shared room', 'room_type_Hotel room',\n",
    "       'cancellation_policy_strict', 'cancellation_policy_flexible', 'host_response_rate_bins_100%',\n",
    "       'bed_type_Real Bed', 'bed_type_Pull-out Sofa',\n",
    "       'bed_type_Airbed', 'bed_type_Couch', 'host_is_superhost',\n",
    "       'host_total_listings_count', 'host_identity_verified', 'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds', 'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people',\n",
    "       'minimum_nights', 'maximum_nights', 'availability_365',\n",
    "       'number_of_reviews_ltm', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'instant_bookable', 'host_days_active_years',\n",
    "       'host_listing_since', 'special_amenities', 'common_amenities',\n",
    "       'avg_price_property_type', 'avg_review_score']]\n",
    "y = final_df['price']\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(f'Coefficients: {lin_reg.coef_}')\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'R^2 score: {lin_reg.score(X, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_constant = sm.add_constant(X)\n",
    "lin_reg = sm.OLS(y,X_constant).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 4 more parameters from the input\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "pd.DataFrame({'vif': vif[0:]}, index=X.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  removed like correlated variables\n",
    "X = final_df[['room_type_Entire home/apt', 'room_type_Private room',\n",
    "       'room_type_Shared room', 'room_type_Hotel room',\n",
    "       'cancellation_policy_strict', 'cancellation_policy_flexible', 'host_response_rate_bins_100%',\n",
    "       'bed_type_Real Bed', 'bed_type_Pull-out Sofa',\n",
    "       'bed_type_Airbed', 'bed_type_Couch', \n",
    "       'host_total_listings_count',  'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds',  'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people',\n",
    "       'minimum_nights', 'maximum_nights', 'availability_365',\n",
    "       'number_of_reviews_ltm', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'instant_bookable', 'host_days_active_years',\n",
    "       'host_listing_since', 'special_amenities', 'common_amenities',\n",
    "       'avg_price_property_type', 'avg_review_score']]\n",
    "y = final_df['price']\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(f'Coefficients: {lin_reg.coef_}')\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'R^2 score: {lin_reg.score(X, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_constant = sm.add_constant(X)\n",
    "lin_reg = sm.OLS(y,X_constant).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 4 more parameters from the input\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "pd.DataFrame({'vif': vif[0:]}, index=X.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  removed like correlated variables\n",
    "X = final_df[['room_type_Entire home/apt',\n",
    "       'room_type_Shared room',\n",
    "       'cancellation_policy_strict', 'cancellation_policy_flexible', 'host_response_rate_bins_100%',\n",
    "       'bed_type_Real Bed', 'bed_type_Pull-out Sofa',\n",
    "       'bed_type_Airbed', 'bed_type_Couch', \n",
    "       'host_total_listings_count',  'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds',  'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people',\n",
    "       'minimum_nights', 'maximum_nights', 'availability_365',\n",
    "       'number_of_reviews_ltm', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'instant_bookable', 'host_days_active_years',\n",
    "       'host_listing_since', 'special_amenities', 'common_amenities',\n",
    "       'avg_price_property_type', 'avg_review_score']]\n",
    "y = final_df['price']\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(f'Coefficients: {lin_reg.coef_}')\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'R^2 score: {lin_reg.score(X, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_constant = sm.add_constant(X)\n",
    "lin_reg = sm.OLS(y,X_constant).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 4 more parameters from the input\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "pd.DataFrame({'vif': vif[0:]}, index=X.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  removed like correlated variables\n",
    "X = final_df[['room_type_Entire home/apt',\n",
    "       'room_type_Shared room',\n",
    "       'cancellation_policy_strict', 'cancellation_policy_flexible', 'host_response_rate_bins_100%',\n",
    "       'bed_type_Real Bed', 'bed_type_Pull-out Sofa',\n",
    "       'bed_type_Airbed', 'bed_type_Couch', \n",
    "       'host_total_listings_count',  'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds',  'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people',\n",
    "       'minimum_nights', 'availability_365',\n",
    "       'number_of_reviews_ltm', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'instant_bookable', 'host_days_active_years',\n",
    "       'host_listing_since', 'special_amenities', 'common_amenities',\n",
    "       'avg_price_property_type', 'avg_review_score']]\n",
    "y = final_df['price']\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(f'Coefficients: {lin_reg.coef_}')\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'R^2 score: {lin_reg.score(X, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_constant = sm.add_constant(X)\n",
    "lin_reg = sm.OLS(y,X_constant).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 4 more parameters from the input\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "pd.DataFrame({'vif': vif[0:]}, index=X.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(pd.DataFrame({'vif': vif[0:]}, index=X.columns).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  removed like correlated variables\n",
    "X = final_df[['room_type_Entire home/apt',\n",
    "       'room_type_Shared room',\n",
    "       'cancellation_policy_strict', 'cancellation_policy_flexible', 'host_response_rate_bins_100%',\n",
    "       'bed_type_Real Bed', 'bed_type_Pull-out Sofa',\n",
    "       'bed_type_Airbed', 'bed_type_Couch', \n",
    "       'host_total_listings_count',  'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds',  'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people',\n",
    "       'minimum_nights', 'availability_365',\n",
    "       'number_of_reviews_ltm', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'instant_bookable', 'host_days_active_years',\n",
    "       'host_listing_since', 'special_amenities', 'common_amenities',\n",
    "       'avg_price_property_type', 'avg_review_score']]\n",
    "y = final_df['price']\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(f'Coefficients: {lin_reg.coef_}')\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'R^2 score: {lin_reg.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally let's check for overfit and underfit condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "model = lin_reg.fit(X_train,y_train)\n",
    "print(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\n",
    "print(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply StandardScaler train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "# libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV, LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Data Ready\n",
    "##  removed like correlated variables\n",
    "X = final_df[['room_type_Entire home/apt',\n",
    "       'room_type_Shared room',\n",
    "       'cancellation_policy_strict', 'cancellation_policy_flexible', 'host_response_rate_bins_100%',\n",
    "       'bed_type_Real Bed', 'bed_type_Pull-out Sofa',\n",
    "       'bed_type_Airbed', 'bed_type_Couch', \n",
    "       'host_total_listings_count',  'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds',  'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people',\n",
    "       'minimum_nights', 'availability_365',\n",
    "       'number_of_reviews_ltm', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'instant_bookable', 'host_days_active_years',\n",
    "       'host_listing_since', 'special_amenities', 'common_amenities',\n",
    "       'avg_price_property_type', 'avg_review_score']]\n",
    "y = final_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x , train_y, test_y = train_test_split(X,y, test_size = 0.30, random_state = 1)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# coefficeints of the trained model\n",
    "print('\\nCoefficient of model :', model.coef_)\n",
    "\n",
    "# intercept of the model\n",
    "print('\\nIntercept of model',model.intercept_)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_train = model.predict(train_x)\n",
    "\n",
    "# Root Mean Squared Error on training dataset\n",
    "rmse_train = mean_squared_error(train_y,predict_train)**(0.5)\n",
    "print('\\nRMSE on train dataset : ', rmse_train)\n",
    "\n",
    "# predict the target on the testing dataset\n",
    "predict_test = model.predict(test_x) \n",
    "\n",
    "# Root Mean Squared Error on testing dataset\n",
    "rmse_test = mean_squared_error(test_y,predict_test)**(0.5)\n",
    "print('\\nRMSE on test dataset : ', rmse_test)\n",
    "\n",
    "print(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\n",
    "print(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "## Raw OLS Model\n",
    "X = final_df.drop(['price'],axis=1)\n",
    "y = final_df.price\n",
    "feature_select = LassoCV(precompute=True)\n",
    "feature_select.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % feature_select.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %feature_select.score(X,y))\n",
    "coef = pd.Series(feature_select.coef_, index = X.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef < 0)) + \" variables\")\n",
    "imp_coef = coef.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef=imp_coef[coef!=0]\n",
    "imp_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = final_df[['minimum_nights','number_of_reviews_ltm','maximum_nights','security_deposit','availability_365','cleaning_fee',\n",
    "                  'review_scores_rating',\n",
    "                  'avg_price_property_type','common_amenities','accommodates','room_type_Entire home/apt']]\n",
    "y_new= final_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainx, testx , trainy, testy = train_test_split(X_new,y_new, test_size = 0.30, random_state = 1)\n",
    "print(trainx.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear reg after feature selectio\n",
    "# fit the model with the training data\n",
    "model.fit(trainx,trainy)\n",
    "\n",
    "# coefficeints of the trained model\n",
    "print('\\nCoefficient of model :', model.coef_)\n",
    "\n",
    "# intercept of the model\n",
    "print('\\nIntercept of model',model.intercept_)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_train = model.predict(trainx)\n",
    "\n",
    "# Root Mean Squared Error on training dataset\n",
    "rmse_train = mean_squared_error(trainy,predict_train)**(0.5)\n",
    "print('\\nRMSE on train dataset : ', rmse_train)\n",
    "\n",
    "# predict the target on the testing dataset\n",
    "predict_test = model.predict(testx) \n",
    "\n",
    "# Root Mean Squared Error on testing dataset\n",
    "rmse_test = mean_squared_error(testy,predict_test)**(0.5)\n",
    "print('\\nRMSE on test dataset : ', rmse_test)\n",
    "\n",
    "print(f'R^2 score for train: {model.score(trainx, trainy)}')\n",
    "print(f'R^2 score for test: {model.score(testx, testy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Data Ready\n",
    "##  removed like correlated variables\n",
    "X = final_df[['room_type_Entire home/apt',\n",
    "       'room_type_Shared room',\n",
    "       'cancellation_policy_strict', 'cancellation_policy_flexible', 'host_response_rate_bins_100%',\n",
    "       'bed_type_Real Bed', 'bed_type_Pull-out Sofa',\n",
    "       'bed_type_Airbed', 'bed_type_Couch', \n",
    "       'host_total_listings_count',  'accommodates',\n",
    "       'bathrooms', 'bedrooms', 'beds',  'security_deposit',\n",
    "       'cleaning_fee', 'Num_of_guests_incl_forprice', 'price_per_extra_people',\n",
    "       'minimum_nights', 'availability_365',\n",
    "       'number_of_reviews_ltm', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'instant_bookable', 'host_days_active_years',\n",
    "       'host_listing_since', 'special_amenities', 'common_amenities',\n",
    "       'avg_price_property_type', 'avg_review_score']]\n",
    "y = final_df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge basic model\n",
    "\n",
    "train_x = X_train\n",
    "train_y = y_train\n",
    "test_x = X_test\n",
    "test_y = y_test\n",
    "\n",
    "rr = Ridge(alpha=0.01)\n",
    "rr.fit(train_x, train_y) \n",
    "pred_train_rr= rr.predict(train_x)\n",
    "print('train_rmse: ',np.sqrt(mean_squared_error(train_y,pred_train_rr)))\n",
    "print('train_r2 score: ',r2_score(train_y, pred_train_rr))\n",
    "print('-------------------------')\n",
    "pred_test_rr= rr.predict(test_x)\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(test_y,pred_test_rr))) \n",
    "print('test_r2 score: ',r2_score(test_y, pred_test_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selected\n",
    "\n",
    "rr = Ridge(alpha=0.01)\n",
    "rr.fit(trainx, trainy) \n",
    "pred_train_rr= rr.predict(trainx)\n",
    "print('ytrain_rmse: ',np.sqrt(mean_squared_error(trainy,pred_train_rr)))\n",
    "print('ytrain_r2 score: ',r2_score(trainy, pred_train_rr))\n",
    "print('-------------------------')\n",
    "pred_test_rr= rr.predict(testx)\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(testy,pred_test_rr))) \n",
    "print('test_r2 score: ',r2_score(testy, pred_test_rr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lasso Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lasso\n",
    "model_lasso = Lasso(alpha=0.01)\n",
    "model_lasso.fit(train_x, train_y) \n",
    "pred_train_lasso= model_lasso.predict(train_x)\n",
    "print('train_rmse: ',np.sqrt(mean_squared_error(train_y,pred_train_lasso)))\n",
    "print('train_r2 score: ',r2_score(train_y, pred_train_lasso))\n",
    "print('-------------------------')\n",
    "pred_test_lasso= model_lasso.predict(test_x)\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(test_y,pred_test_lasso))) \n",
    "print('test_r2 score: ',r2_score(test_y, pred_test_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selected\n",
    "model_lasso = Lasso(alpha=0.01)\n",
    "model_lasso.fit(trainx, trainy) \n",
    "pred_train_lasso= model_lasso.predict(trainx)\n",
    "print('train_rmse: ',np.sqrt(mean_squared_error(trainy,pred_train_lasso)))\n",
    "print('train_r2 score: ',r2_score(trainy, pred_train_lasso))\n",
    "print('-------------------------')\n",
    "pred_test_lasso= model_lasso.predict(testx)\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(testy,pred_test_lasso))) \n",
    "print('test_r2 score: ',r2_score(testy, pred_test_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elastic Net\n",
    "model_enet = ElasticNet(alpha = 0.01)\n",
    "model_enet.fit(trainx, trainy) \n",
    "pred_train_enet= model_enet.predict(trainx)\n",
    "print('train_rmse: ',np.sqrt(mean_squared_error(trainy,pred_train_enet)))\n",
    "print('train_r2 score: ',r2_score(trainy, pred_train_enet))\n",
    "\n",
    "pred_test_enet= model_enet.predict(testx)\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(testy,pred_test_enet)))\n",
    "print('test_r2 score: ',r2_score(testy, pred_test_enet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "# define the model\n",
    "model = GradientBoostingRegressor()\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting ensemble for making predictions for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "# define the model\n",
    "model = GradientBoostingRegressor()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(trainx, trainy)\n",
    "# make a single prediction\n",
    "\n",
    "yhat = model.predict(testx)\n",
    "# summarize prediction\n",
    "\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(testy,yhat)))\n",
    "print('test_r2 score: ',r2_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF selected\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "# define the model\n",
    "model = GradientBoostingRegressor()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(train_x, train_y)\n",
    "# make a single prediction\n",
    "\n",
    "yhat = model.predict(test_x)\n",
    "# summarize prediction\n",
    "\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(test_y,yhat)))\n",
    "print('test_r2 score: ',r2_score(test_y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "xgb_model = xgboost.XGBRegressor(\n",
    "                 max_depth=3,\n",
    "                 n_estimators=100,                                                    \n",
    "                 seed=42)\n",
    "xgb_model.fit(train_x,train_y)\n",
    "yhat = xgb_model.predict(test_x)\n",
    "# summarize prediction\n",
    "\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(test_y,yhat)))\n",
    "print('test_r2 score: ',r2_score(test_y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "xgb_model = xgboost.XGBRegressor(\n",
    "                 max_depth=3,\n",
    "                 n_estimators=100,                                                    \n",
    "                 seed=42)\n",
    "xgb_model.fit(trainx,trainy)\n",
    "yhat = xgb_model.predict(testx)\n",
    "# summarize prediction\n",
    "\n",
    "print('test_rmse: ',np.sqrt(mean_squared_error(testy,yhat)))\n",
    "print('test_r2 score: ',r2_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, title):\n",
    "    predictions = model.predict(X)\n",
    "    errors = abs(np.expm1(predictions) - np.expm1(y))\n",
    "    mape = 100 * np.mean(errors / np.expm1(y))\n",
    "    accuracy = 100 - mape\n",
    "    score_gbr = model.score(X,y)\n",
    "    #rsquared = r2_score(y,predictions)\n",
    "    rmse_gbr = np.sqrt(mean_squared_error((y),(predictions)))\n",
    "    \n",
    "    print(title)\n",
    "    print('R^2: {:0.4f}'.format(score_gbr))\n",
    "#     print('R^2: {:0.4f}'.format(rsquared))\n",
    "    print('RMSE: ${:0.4f} '.format(rmse_gbr))\n",
    "#     print('Average Error: ${:0.4f}'.format(np.mean(errors)))\n",
    "#     print('Accuracy = {:0.3f}%.'.format(accuracy),'\\n')\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "    \n",
    "def scatter_plot(prediction,y,title):\n",
    "    plt.rcParams['figure.figsize']=(10,4)\n",
    "    plt.style.use(style='ggplot')\n",
    "    plt.scatter(x=prediction, y=y, alpha=.75)\n",
    "    plt.ylabel('log(input price)',fontsize=16)\n",
    "    plt.xlabel('log(predicted price)',fontsize=16)\n",
    "    plt.tick_params(labelsize=16)\n",
    "    plt.title(title,fontsize=16)\n",
    "    plt.show()    \n",
    "    \n",
    "def feature_extraction(importances,title):\n",
    "    plt.rcParams['figure.figsize']=(12,6)\n",
    "#     importances[0:15].iloc[::-1].plot(kind='barh',legend=False,fontsize=16)\n",
    "#     #importances.plot(kind='barh',legend=False,fontsize=16)\n",
    "# #     plt.tick_params(labelsize=18)\n",
    "# #     plt.ylabel(\"Feature\",fontsize=20)\n",
    "# #     plt.xlabel(\"Importance viariable\",fontsize=20)\n",
    "# #     plt.title(title,fontsize=20)\n",
    "#     plt.show()\n",
    "    \n",
    "def scatter_plot2(prediction1,y1,prediction2,y2,title):\n",
    "    a=min(min(prediction1),min(y1),min(prediction2),min(y2))-0.2\n",
    "    b=max(max(prediction1),max(y1),max(prediction2),max(y2))+0.2\n",
    "    plt.rcParams['figure.figsize']=(10,4)\n",
    "    plt.style.use(style='ggplot')\n",
    "    plt.scatter(x=prediction1, y=prediction1-y1, color='red',label='Training data',alpha=.75)\n",
    "    plt.scatter(x=prediction2, y=prediction2-y2, color='blue', marker='s', label='Test data',alpha=.75)\n",
    "    plt.hlines(y = 0, xmin = a, xmax = b, color = \"black\")\n",
    "    plt.ylabel('log(input price)',fontsize=16)\n",
    "    plt.xlabel('log(predicted price)',fontsize=16)\n",
    "    plt.tick_params(labelsize=16)\n",
    "    plt.title(title,fontsize=16)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.show()    \n",
    "def scatter_plot3(prediction1,y1,prediction2,y2,title):\n",
    "    a=min(min(prediction1),min(y1),min(prediction2),min(y2))-0.2\n",
    "    b=max(max(prediction1),max(y1),max(prediction2),max(y2))+0.2\n",
    "    plt.rcParams['figure.figsize']=(10,4)\n",
    "    plt.style.use(style='ggplot')\n",
    "    plt.scatter(x=prediction1, y=y1, color='red',label='Training data',alpha=.75)\n",
    "    plt.scatter(x=prediction2, y=y2, color='blue', marker='s', label='Test data',alpha=.75)\n",
    "    plt.plot([a, b], [a, b], c = \"black\")\n",
    "    plt.ylabel('log(input price)',fontsize=16)\n",
    "    plt.xlabel('log(predicted price)',fontsize=16)\n",
    "    plt.tick_params(labelsize=16)\n",
    "    plt.title(title,fontsize=16)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestRegressor(random_state=1, n_jobs=-2, max_features='log2')\n",
    "\n",
    "param_grid = dict(n_estimators=[100,50],\n",
    "                  max_depth=[None,3],\n",
    "                  min_samples_leaf=[1,2])\n",
    "\n",
    "grid_rf=GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_rf.fit(X_train,y_train)\n",
    "\n",
    "#print(\"Random forest grid.cv_results_ {}\".format(grid_rf.cv_results_))\n",
    "print(\"Random forest grid.best_score_ {}\".format(grid_rf.best_score_))\n",
    "print(\"Random forest grid.best_params_ {}\".format(grid_rf.best_params_))\n",
    "print(\"Random forest grid.best_estimator_ {}\".format(grid_rf.best_estimator_))\n",
    "\n",
    "model_rf = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title0='Random Forest Regression:'\n",
    "model_tmp = model_rf\n",
    "\n",
    "title=title0 + ' training set model performance'\n",
    "prediction_train=evaluate(model_tmp, X_train, y_train,title)\n",
    "\n",
    "title=title0 + ' test set model performance'\n",
    "prediction_test=evaluate(model_tmp, X_test, y_test,title)\n",
    "\n",
    "\n",
    "scatter_plot2(prediction_train,y_train,prediction_test,y_test,title)\n",
    "\n",
    "title=title0 + ' performance evaluation'\n",
    "scatter_plot3(prediction_train,y_train,prediction_test,y_test,title)\n",
    "\n",
    "importances_train = pd.DataFrame({'Feature':X_train.columns, 'Importance':model_rf.feature_importances_})\n",
    "importances_train = importances_train.sort_values('Importance',ascending=False).set_index('Feature')\n",
    "feature_extraction(importances_train,'Random Forest Regression: Training set feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting hyperparameter tuned\n",
    "# Hyperparameter tuned gradient boosting Regression\n",
    "gbr = GradientBoostingRegressor(min_samples_split=2,\n",
    "                                min_samples_leaf=2,\n",
    "                                subsample=0.8,\n",
    "                                random_state=1,\n",
    "                               learning_rate=0.01,\n",
    "                               max_features='sqrt')\n",
    "#param_grid = {\"n_estimators\":np.arange(1000,10000,1000),'learning_rate':[0.01,0.05,0.1,0.25,0.5]}\n",
    "param_grid = dict(n_estimators=[100,600], max_depth=[1,3,8])\n",
    "\n",
    "grid_gbr=GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error',n_jobs=-2)\n",
    "\n",
    "grid_gbr.fit(X_train,y_train)\n",
    "\n",
    "#print(\"Random forest grid.cv_results_ {}\".format(grid_gbr.cv_results_))\n",
    "print(\"Random forest grid.best_score_ {}\".format(grid_gbr.best_score_))\n",
    "print(\"Random forest grid.best_params_ {}\".format(grid_gbr.best_params_))\n",
    "print(\"Random forest grid.best_estimator_ {}\".format(grid_gbr.best_estimator_))\n",
    "\n",
    "model_gbr = grid_gbr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title0='Gradient Boosting Regression:'\n",
    "model_tmp = model_gbr\n",
    "\n",
    "title=title0 + ' training set model performance'\n",
    "prediction_train=evaluate(model_tmp, X_train, y_train,title)\n",
    "\n",
    "title=title0 + ' test set model performance'\n",
    "prediction_test=evaluate(model_tmp, X_test, y_test,title)\n",
    "\n",
    "title=title0 + ' residual plot'\n",
    "scatter_plot2(prediction_train,y_train,prediction_test,y_test,title)\n",
    "\n",
    "title=title0 + ' performance evaluation'\n",
    "scatter_plot3(prediction_train,y_train,prediction_test,y_test,title)\n",
    "\n",
    "importances_train = pd.DataFrame({'Feature':X_train.columns, 'Importance':model_tmp.feature_importances_})\n",
    "importances_train = importances_train.sort_values('Importance',ascending=False).set_index('Feature')\n",
    "feature_extraction(importances_train,'Gradient Boosting Regression: Training set feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL REPORT :\n",
    "\n",
    "**Linear Reg**\n",
    "RFE Feature selection:\n",
    "\n",
    "R^2 score for train: 0.6369799616222565\n",
    "R^2 score for test: 0.635412119637978\n",
    "\n",
    "After applying Standard scaler:\n",
    "\n",
    "R^2 score for train: 0.6369799616222565\n",
    "R^2 score for test: 0.635412119637978\n",
    "\n",
    "**Ridge reg:**\n",
    "\n",
    "RFE Feature selection:\n",
    "\n",
    "train_rmse:  27.063829839500183\n",
    "train_r2 score:  0.6369799616214836\n",
    "test_rmse:  27.122013030869358\n",
    "test_r2 score:  0.6354121243730774\n",
    "\n",
    "Lasso cv Feature selection:\n",
    " \n",
    "ytrain_rmse:  27.262963919522388\n",
    "ytrain_r2 score:  0.6316181474690495\n",
    "test_rmse:  27.31931774864737\n",
    "test_r2 score:  0.6300882894100244\n",
    "\n",
    "\n",
    "**Lasso Reg:**\n",
    "\n",
    "RFE Feature selection:\n",
    "\n",
    "train_rmse:  27.06526953410705\n",
    "train_r2 score:  0.6369413379754045\n",
    "test_rmse:  27.121916828518213\n",
    "test_r2 score:  0.6354147107702295\n",
    "\n",
    "Lasso cv Feature selection:\n",
    "\n",
    "train_rmse:  27.26297648274075\n",
    "train_r2 score:  0.6316178079562413\n",
    "test_rmse:  27.319375560168037\n",
    "test_r2 score:  0.6300867238379024\n",
    "\n",
    "**Elastic_Net:**\n",
    "RFE Feature selection:\n",
    "\n",
    "train_rmse:  27.26657574369064\n",
    "train_r2 score:  0.6315205338260843\n",
    "test_rmse:  27.323278460597063\n",
    "test_r2 score:  0.6299810231926983\n",
    "\n",
    "**Gradient Boosting**\n",
    "\n",
    "RFE Feature selection:\n",
    "\n",
    "test_rmse:  25.981896947224925\n",
    "test_r2 score:  0.6654199102825642\n",
    "\n",
    "Lasso cv Feature selection:\n",
    "\n",
    "test_rmse:  25.49278693944052\n",
    "test_r2 score:  0.677898302118356\n",
    "\n",
    "\n",
    "\n",
    "**XG Boost:**\n",
    "\n",
    "RFE Feature selection:\n",
    "\n",
    "test_rmse:  24.90947071759183\n",
    "test_r2 score:  0.6924700760810385\n",
    "\n",
    "Lasso cv Feature selection:\n",
    "\n",
    "test_rmse:  25.593941826859893\n",
    "test_r2 score:  0.6753370440057858\n",
    "\n",
    "\n",
    "**Random Forest:**\n",
    "\n",
    "Random Forest Regression: training set model performance\n",
    "R^2: 0.9761\n",
    "Random Forest Regression: test set model performance\n",
    "R^2: 0.8481\n",
    "\n",
    "**Gradient Boosting Regression: training set model performance:**\n",
    "R^2: 0.7548\n",
    "\n",
    "Gradient Boosting Regression: test set model performance\n",
    "R^2: 0.7272"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
